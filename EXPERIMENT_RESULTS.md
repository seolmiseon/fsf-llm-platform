# FSF 프로젝트 실험 결과

## 실험 개요
- **목적**: 프롬프트 엔지니어링 및 캐싱 전략을 통한 성능 개선 검증
- **측정 일시**: 2025-12-23 (프롬프트 엔지니어링), 실제 운영 데이터 (캐싱)
- **프로젝트**: FSF (Football Statistics & Fan) - 축구 분석 AI 플랫폼

---

## 실험 1: 프롬프트 엔지니어링 (Agent 성능 개선)

### 실험 목적
Agent 안정성 및 정답률 향상을 위한 프롬프트 최적화

### 실험 조건
- **Temperature**: 0.7
- **Max Tokens**: 1000
- **프롬프트 전략**: ReAct 프롬프트 + 하이브리드 질문 분류
- **테스트 쿼리 수**: 47개 (단순 질문 20개, 복잡 질문 27개)

### 측정 결과

| 지표 | 결과 |
|------|:----:|
| **전체 정답률** | **97.9%** (46/47) |
| **단순 질문 정답률** | 95.0% (19/20) |
| **복잡 질문 정답률** | **100%** (27/27) |
| **축약형 감지율** | 80.0% (4/5) |

### 주요 개선 사항

#### 리팩토링 전
- 단순/복잡 질문 구분 없이 모든 질문을 Agent로 처리
- 정확도: 90-95%
- LLM 호출: 모든 질문에 대해 2회 호출 (비용 높음)

#### 리팩토링 후
- **하이브리드 질문 분류**: 단순 질문은 chat.py (LLM 1회 호출), 복잡 질문만 Agent (LLM 2회 호출)
- **ReAct 프롬프트**: "Thought → Action → Observation" 구조로 추론 과정 명확화
- **정규식 기반 빠른 판단**: 비교 키워드("vs", "비교"), 경기 ID 패턴, 축약형 질문 감지
- **정확도**: 90-95% → **97.9%** (축약형도 감지)

### 기술적 구현

1. **질문 분류 로직** (`question_classifier.py`)
   - 정규식 기반 빠른 판단 (비용 $0)
   - 애매한 경우만 LLM 호출 (정확도 우선)
   - 결과 캐시 (메모리 기반, 24시간 TTL)

2. **ReAct 프롬프트** (`agent.py`)
   - Thought → Action → Observation 구조 강제
   - Tool 선택 정확도 향상
   - 에러 복구 로직 포함

3. **하이브리드 분기**
   - 단순 질문: `chat.py` 사용 (LLM 1회 호출, 저렴)
   - 복잡 질문: Agent 사용 (LLM 2회 호출, 정확도 우선)

### 성과

- **정확도 향상**: 90-95% → **97.9%** (+2.9~7.9%p)
- **복잡 질문 정확도**: **100%** (27/27)
- **비용 최적화**: 단순 질문은 LLM 1회 호출로 절감
- **축약형 감지**: "맨유 토트넘", "손흥민 홀란드" 등 축약형 질문도 정확히 감지

### 한계 및 개선 방향

- **한계**:
  - 축약형 감지율 80% (5개 중 4개) - 추가 개선 필요
  - 단순 질문 정확도 95% (20개 중 19개) - 1개 오분류

- **다음 액션**:
  - 다른 LLM(Claude, Gemini)으로 동일 실험 진행
  - Few-shot 예제 수 최적화 (3개 → 5개) 실험
  - ReAct 프롬프트 템플릿 일반화 및 재사용 가능한 형태로 정리

---

## 실험 2: 캐싱 전략 효과 측정

### 실험 목적
2단계 캐싱으로 응답 속도 및 비용 최적화

### 실험 조건
- **캐싱 전략**: ChromaDB (벡터 검색) + Firestore (API 캐시)
- **측정 방법**: 100개 반복 질문으로 캐시 히트율 측정
- **측정 기간**: 실제 운영 데이터

### 측정 결과

| 지표 | 캐싱 없음 | ChromaDB + Firestore | 개선율 |
|------|:---------:|:--------------------:|:------:|
| **평균 응답 시간** | 350ms | **50ms** | **7배 향상** |
| **캐시 히트율** | 0% | **90%** | - |
| **API 비용 절감** | - | **40%** | - |
| **월 비용** | $0.07 (초기) | **$0.05** (3개월 후) | - |

### 주요 개선 사항

#### 리팩토링 전
- 캐싱 없음
- 모든 질문에 대해 OpenAI API 호출
- 평균 응답 시간: 350ms

#### 리팩토링 후
- **1단계: ChromaDB 벡터 캐시** - 유사 질문 재사용 (유사도 0.75 이상 캐시 후보)
- **2단계: Firestore 캐시** - 외부 API 응답 캐시 (1시간 TTL)
- **Judge 노드**: 캐시 데이터 충분성 판단 (LLM 1회 호출, 정확도 향상)
- **Keyword 매칭**: 핵심 키워드 일치 여부로 Judge 호출 최소화

### 기술적 구현

1. **ChromaDB 벡터 캐시** (`cache_service.py`)
   - 유사도 0.75 이상 캐시 후보
   - 유사도 0.9 이상: Judge 스킵 (비용 절감)
   - 유사도 0.7~0.9: Judge 노드에서 최종 판단

2. **Firestore API 캐시**
   - 외부 API 응답 캐시 (1시간 TTL)
   - 실시간 정보 업데이트 시 자동 무효화

3. **Judge 노드** (`cache_judge.py`)
   - ReAct 방식: "생각 → 판단 → 이유" 구조
   - 캐시 데이터 충분성 판단
   - Hallucination 방지

4. **Keyword 매칭** (`keyword_matcher.py`)
   - 핵심 키워드 추출 및 매칭
   - Keyword 점수 < 0.5면 Judge 스킵

### 성과

- **응답 시간**: 350ms → **50ms** (7배 향상)
- **캐시 히트율**: 0% → **90%**
- **API 비용 절감**: **40%**
- **월 비용**: $0.07 → **$0.05** (캐시 히트율 증가로 추가 절감)

### 한계 및 개선 방향

- **한계**:
  - 캐시 유효기간 1시간으로 설정 (실시간성 vs 성능 트레이드오프)
  - ChromaDB 유사도 임계값 0.75로 설정 (너무 높으면 캐시 미스 증가, 너무 낮으면 부정확한 답변 위험)
  - Judge 노드 추가로 LLM 호출 1회 증가 (비용 증가하나 정확도 향상)

- **다음 액션**:
  - Redis 추가하여 3단계 캐싱 전략 실험 (메모리 캐시 → ChromaDB → Firestore)
  - 캐시 TTL 동적 조정 (데이터 타입별 차등 적용)
  - 캐시 무효화 전략 개선 (실시간 정보 업데이트 시 자동 무효화)

---

## 결론

1. **프롬프트 엔지니어링**: 정확도 90-95% → **97.9%** 향상
2. **캐싱 전략**: 응답 시간 350ms → **50ms** (7배 향상), 비용 **40%** 절감
3. **기술적 완성도**: 하이브리드 질문 분류 + 2단계 캐싱으로 성능과 비용 최적화

---

**측정 도구**: 
- 프롬프트 엔지니어링: `server/test_question_classifier.py`
- 캐싱 전략: 실제 운영 데이터

**측정 환경**: 
- Python 3.10, FastAPI, LangChain, OpenAI GPT-4o-mini
- ChromaDB (벡터 검색), Firestore (API 캐시)

