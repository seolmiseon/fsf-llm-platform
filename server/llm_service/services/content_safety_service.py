"""
Content Safety Service
ìœ í•´ ì½˜í…ì¸  ë° ê¸ˆì§€ì–´ í•„í„°ë§ ì„œë¹„ìŠ¤

ê¸°ëŠ¥:
1. ì…ë ¥ ê²Œì´íŠ¸ì›¨ì´: ì‚¬ìš©ì ì¿¼ë¦¬ í•„í„°ë§
2. ì¶œë ¥ í•„í„°: LLM ì‘ë‹µ í•„í„°ë§
3. ì»¤ìŠ¤í…€ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì§€ì›
4. LLM ê¸°ë°˜ ì •êµí•œ ê°ì§€ ë° ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
5. í–¥í›„ GCP Content Safety API í†µí•© ê°€ëŠ¥
"""

import re
import logging
import json
import os
from typing import Dict, List, Optional, Tuple
from enum import Enum
from openai import OpenAI

logger = logging.getLogger(__name__)


class ContentCategory(str, Enum):
    """ì½˜í…ì¸  ì¹´í…Œê³ ë¦¬"""
    HARMFUL = "harmful"  # ìœ í•´ ì½˜í…ì¸ 
    PROFANITY = "profanity"  # ìš•ì„¤
    HATE_SPEECH = "hate_speech"  # í˜ì˜¤ ë°œì–¸
    SPAM = "spam"  # ìŠ¤íŒ¸
    PERSONAL_INFO = "personal_info"  # ê°œì¸ì •ë³´
    INAPPROPRIATE = "inappropriate"  # ë¶€ì ì ˆí•œ ë‚´ìš©


class ContentSafetyResult:
    """ì½˜í…ì¸  ì•ˆì „ì„± ê²€ì‚¬ ê²°ê³¼"""
    def __init__(
        self,
        is_safe: bool,
        category: Optional[ContentCategory] = None,
        detected_words: Optional[List[str]] = None,
        reason: Optional[str] = None
    ):
        self.is_safe = is_safe
        self.category = category
        self.detected_words = detected_words or []
        self.reason = reason

    def to_dict(self) -> Dict:
        return {
            "is_safe": self.is_safe,
            "category": self.category.value if self.category else None,
            "detected_words": self.detected_words,
            "reason": self.reason
        }


class ContentSafetyService:
    """ì½˜í…ì¸  ì•ˆì „ì„± ê²€ì‚¬ ì„œë¹„ìŠ¤"""
    
    def __init__(self, use_llm: bool = True):
        """ì´ˆê¸°í™”: ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë° íŒ¨í„´ ë¡œë“œ"""
        # ì»¤ìŠ¤í…€ ë¸”ë™ë¦¬ìŠ¤íŠ¸ (í”„ë¡œì íŠ¸ ê³ ìœ  ê·œì¹™)
        self.custom_blacklist = self._load_custom_blacklist()
        
        # ìœ í•´ ì½˜í…ì¸  íŒ¨í„´ (í•œê¸€/ì˜ë¬¸)
        self.harmful_patterns = self._load_harmful_patterns()
        
        # ìŠ¤íŒ¸ íŒ¨í„´
        self.spam_patterns = self._load_spam_patterns()
        
        # LLM ê¸°ë°˜ ê°ì§€ í™œì„±í™” ì—¬ë¶€
        self.use_llm = use_llm
        self.llm_client = None
        
        if self.use_llm:
            try:
                api_key = os.getenv("OPENAI_API_KEY")
                if api_key:
                    self.llm_client = OpenAI(api_key=api_key)
                    self.llm_model = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini")
                    logger.info("âœ… LLM ê¸°ë°˜ ì½˜í…ì¸  ê°ì§€ í™œì„±í™”")
                else:
                    logger.warning("âš ï¸ OPENAI_API_KEYê°€ ì—†ì–´ LLM ê¸°ë°˜ ê°ì§€ ë¹„í™œì„±í™”")
                    self.use_llm = False
            except Exception as e:
                logger.warning(f"âš ï¸ LLM í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
                self.use_llm = False
        
        logger.info("âœ… Content Safety Service ì´ˆê¸°í™” ì™„ë£Œ")

    def _load_custom_blacklist(self) -> List[str]:
        """
        ì»¤ìŠ¤í…€ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë¡œë“œ
        í”„ë¡œì íŠ¸ ê³ ìœ ì˜ ê¸ˆì§€ì–´ ëª©ë¡ (ì˜ˆ: íŠ¹ì • ê³ ê°ì‚¬ ì´ë¦„, ê¸°ë°€ ì •ë³´ ë“±)
        """
        # TODO: í™˜ê²½ë³€ìˆ˜ë‚˜ ì„¤ì • íŒŒì¼ì—ì„œ ë¡œë“œ ê°€ëŠ¥
        return [
            # ì˜ˆì‹œ: í”„ë¡œì íŠ¸ íŠ¹ì • ê¸ˆì§€ì–´
            # "ê¸°ë°€ì •ë³´",
            # "ë‚´ë¶€ë¬¸ì„œ",
        ]

    def _load_harmful_patterns(self) -> Dict[ContentCategory, List[str]]:
        """ìœ í•´ ì½˜í…ì¸  íŒ¨í„´ ë¡œë“œ"""
        return {
            ContentCategory.PROFANITY: [
                # í•œê¸€ ìš•ì„¤ íŒ¨í„´ (ì¼ë¶€ ì˜ˆì‹œ)
                r"(ì‹œ|ì”¨)ë°œ",
                r"(ë³‘|ë¹™)ì‹ ",
                r"(ì¢†|ì¢ƒ)",
                r"ê°œìƒˆ",
                # ì˜ë¬¸ ìš•ì„¤ (ì¼ë¶€ ì˜ˆì‹œ)
                r"\b(fuck|shit|damn|bitch|asshole)\b",
            ],
            ContentCategory.HATE_SPEECH: [
                # í˜ì˜¤ ë°œì–¸ íŒ¨í„´
                r"(ë™|ì„œ|ë‚¨|ë¶)ë…",
                r"ì¼ë³¸ë†ˆ",
                r"ì¤‘êµ­ë†ˆ",
            ],
            ContentCategory.HARMFUL: [
                # ìí•´/í­ë ¥ ê´€ë ¨
                r"ìì‚´",
                r"ìí•´",
                r"ì‚´ì¸",
                r"í­íƒ„",
                r"í…ŒëŸ¬",
            ],
            ContentCategory.INAPPROPRIATE: [
                # ì„±ì¸ ì½˜í…ì¸  ê´€ë ¨
                r"ì„±ì¸ì‚¬ì´íŠ¸",
                r"í¬ë¥´ë…¸",
            ],
        }

    def _load_spam_patterns(self) -> List[str]:
        """ìŠ¤íŒ¸ íŒ¨í„´ ë¡œë“œ"""
        return [
            # ê´‘ê³ /ìŠ¤íŒ¸ íŒ¨í„´
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+",  # URL
            r"[0-9]{3,}-[0-9]{3,}-[0-9]{4,}",  # ì „í™”ë²ˆí˜¸
            r"[ê°€-í£]*íŒë§¤",  # ì¤‘ê³ íŒë§¤
            r"[ê°€-í£]*ë§Œë‚¨",  # ë§Œë‚¨ ìœ ë„
            r"[ê°€-í£]*ê´‘ê³ ",
            r"[ê°€-í£]*í™ë³´",
            r"[ê°€-í£]*ê±°ë˜",  # ê±°ë˜ ìœ ë„
            r"[ê°€-í£]*êµ¬ë§¤",  # êµ¬ë§¤ ìœ ë„
            r"[ê°€-í£]*íŒë§¤.*[ê°€-í£]*",  # íŒë§¤ ê´€ë ¨ ë¬¸êµ¬
            r"ì¹´í†¡|ì¹´ì¹´ì˜¤í†¡|ë¬¸ì˜|ì—°ë½",  # ì—°ë½ì²˜ ìš”ì²­
        ]

    def check_input(self, text: str, use_llm_fallback: bool = True) -> ContentSafetyResult:
        """
        ì…ë ¥ ê²Œì´íŠ¸ì›¨ì´: ì‚¬ìš©ì ì¿¼ë¦¬ í•„í„°ë§
        
        Args:
            text: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
            use_llm_fallback: ì •ê·œì‹ì—ì„œ ê°ì§€ë˜ì§€ ì•Šì•˜ì„ ë•Œ LLM ì²´í¬ ìˆ˜í–‰ ì—¬ë¶€
            
        Returns:
            ContentSafetyResult: ê²€ì‚¬ ê²°ê³¼
        """
        if not text or not text.strip():
            return ContentSafetyResult(
                is_safe=True,
                reason="ë¹ˆ ì…ë ¥"
            )

        text_lower = text.lower()
        detected_words = []

        # 1. ì»¤ìŠ¤í…€ ë¸”ë™ë¦¬ìŠ¤íŠ¸ ì²´í¬
        for word in self.custom_blacklist:
            if word.lower() in text_lower:
                detected_words.append(word)
                return ContentSafetyResult(
                    is_safe=False,
                    category=ContentCategory.HARMFUL,
                    detected_words=[word],
                    reason=f"ì»¤ìŠ¤í…€ ë¸”ë™ë¦¬ìŠ¤íŠ¸ì— í¬í•¨ëœ ë‹¨ì–´ ê°ì§€: {word}"
                )

        # 2. ìœ í•´ ì½˜í…ì¸  íŒ¨í„´ ì²´í¬
        for category, patterns in self.harmful_patterns.items():
            for pattern in patterns:
                try:
                    matches = re.findall(pattern, text_lower, re.IGNORECASE)
                    if matches:
                        detected_words.extend(matches)
                        return ContentSafetyResult(
                            is_safe=False,
                            category=category,
                            detected_words=list(set(matches)),
                            reason=f"{category.value} íŒ¨í„´ ê°ì§€"
                        )
                except re.error as e:
                    logger.warning(f"âš ï¸ ì •ê·œì‹ íŒ¨í„´ ì˜¤ë¥˜ (ë¬´ì‹œ): {pattern} - {e}")
                    continue

        # 3. ìŠ¤íŒ¸ íŒ¨í„´ ì²´í¬
        for pattern in self.spam_patterns:
            try:
                matches = re.findall(pattern, text_lower, re.IGNORECASE)
                if matches:
                    detected_words.extend(matches)
                    return ContentSafetyResult(
                        is_safe=False,
                        category=ContentCategory.SPAM,
                        detected_words=list(set(matches)),
                        reason="ìŠ¤íŒ¸ íŒ¨í„´ ê°ì§€"
                    )
            except re.error as e:
                logger.warning(f"âš ï¸ ì •ê·œì‹ íŒ¨í„´ ì˜¤ë¥˜ (ë¬´ì‹œ): {pattern} - {e}")
                continue

        # 4. LLM ê¸°ë°˜ ì •êµí•œ ê°ì§€ (ì •ê·œì‹ì—ì„œ ê°ì§€ë˜ì§€ ì•Šì€ ê²½ìš°)
        if self.use_llm and use_llm_fallback and self.llm_client:
            try:
                llm_result = self._check_with_llm(text)
                if not llm_result.is_safe:
                    logger.info(f"ğŸ¤– LLM ê¸°ë°˜ ìœ í•´ ì½˜í…ì¸  ê°ì§€: {llm_result.category}")
                    return llm_result
            except Exception as e:
                logger.warning(f"âš ï¸ LLM ê¸°ë°˜ ê°ì§€ ì‹¤íŒ¨ (ì •ê·œì‹ ê²°ê³¼ ì‚¬ìš©): {e}")

        # ì•ˆì „í•œ ì½˜í…ì¸ 
        return ContentSafetyResult(
            is_safe=True,
            reason="ì•ˆì „í•œ ì½˜í…ì¸ "
        )
    
    def _check_with_llm(self, text: str) -> ContentSafetyResult:
        """
        LLMì„ ì‚¬ìš©í•œ ì •êµí•œ ì½˜í…ì¸  ì•ˆì „ì„± ê²€ì‚¬
        
        Args:
            text: ê²€ì‚¬í•  í…ìŠ¤íŠ¸
            
        Returns:
            ContentSafetyResult: ê²€ì‚¬ ê²°ê³¼
        """
        if not self.llm_client:
            return ContentSafetyResult(is_safe=True, reason="LLM ë¹„í™œì„±í™”")
        
        try:
            prompt = f"""ë‹¤ìŒ í…ìŠ¤íŠ¸ê°€ ë¶€ì ì ˆí•œ ë‚´ìš©ì„ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ ë¶„ì„í•´ì£¼ì„¸ìš”.

í…ìŠ¤íŠ¸: "{text}"

ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê±°ë‚˜, ì•ˆì „í•œ ê²½ìš° "safe"ë¥¼ ë°˜í™˜í•˜ì„¸ìš”:
- profanity: ìš•ì„¤, ë¹„ì†ì–´
- hate_speech: í˜ì˜¤ ë°œì–¸, ì°¨ë³„ì  í‘œí˜„
- spam: ìŠ¤íŒ¸, ê´‘ê³ , ì¤‘ê³ íŒë§¤, ë§Œë‚¨ ìœ ë„, ê±°ë˜ ìœ ë„
- harmful: ìí•´, í­ë ¥, ìœ„í—˜í•œ í–‰ë™ ìœ ë„
- inappropriate: ì„±ì¸ ì½˜í…ì¸ , ë¶€ì ì ˆí•œ ë‚´ìš©
- personal_info: ê°œì¸ì •ë³´ ìš”ì²­ ë˜ëŠ” ë…¸ì¶œ
- safe: ì•ˆì „í•œ ì½˜í…ì¸ 

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{{
    "is_safe": true/false,
    "category": "ì¹´í…Œê³ ë¦¬ëª… ë˜ëŠ” safe",
    "reason": "ê°ì§€ ì´ìœ  (í•œêµ­ì–´)",
    "detected_phrases": ["ê°ì§€ëœ êµ¬ë¬¸1", "ê°ì§€ëœ êµ¬ë¬¸2"]
}}"""

            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì½˜í…ì¸  ì•ˆì „ì„± ê²€ì‚¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê³  ê°ê´€ì ìœ¼ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=200,
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                # JSON ì½”ë“œ ë¸”ë¡ ì œê±°
                if "```json" in response_text:
                    response_text = response_text.split("```json")[1].split("```")[0].strip()
                elif "```" in response_text:
                    response_text = response_text.split("```")[1].split("```")[0].strip()
                
                result_dict = json.loads(response_text)
                
                is_safe = result_dict.get("is_safe", True)
                category_str = result_dict.get("category", "safe")
                reason = result_dict.get("reason", "")
                detected_phrases = result_dict.get("detected_phrases", [])
                
                if category_str == "safe" or is_safe:
                    return ContentSafetyResult(
                        is_safe=True,
                        reason=reason or "LLM ê²€ì‚¬ ê²°ê³¼ ì•ˆì „"
                    )
                
                # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
                category = None
                try:
                    category = ContentCategory(category_str)
                except ValueError:
                    # ë§¤í•‘ë˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬ëŠ” INAPPROPRIATEë¡œ ì²˜ë¦¬
                    category = ContentCategory.INAPPROPRIATE
                
                return ContentSafetyResult(
                    is_safe=False,
                    category=category,
                    detected_words=detected_phrases,
                    reason=reason or f"LLM ê¸°ë°˜ {category_str} ê°ì§€"
                )
                
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸ LLM ì‘ë‹µ JSON íŒŒì‹± ì‹¤íŒ¨: {response_text}, ì˜¤ë¥˜: {e}")
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì•ˆì „í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼
                return ContentSafetyResult(
                    is_safe=True,
                    reason="LLM ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"
                )
                
        except Exception as e:
            logger.error(f"âŒ LLM ê¸°ë°˜ ê°ì§€ ì˜¤ë¥˜: {e}", exc_info=True)
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì•ˆì „í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼ (í˜ì¼-ì„¸ì´í”„)
            return ContentSafetyResult(
                is_safe=True,
                reason=f"LLM ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}"
            )

    def check_output(self, text: str) -> ContentSafetyResult:
        """
        ì¶œë ¥ í•„í„°: LLM ì‘ë‹µ í•„í„°ë§
        
        Args:
            text: LLM ì‘ë‹µ í…ìŠ¤íŠ¸
            
        Returns:
            ContentSafetyResult: ê²€ì‚¬ ê²°ê³¼
        """
        # ì¶œë ¥ í•„í„°ëŠ” ì…ë ¥ í•„í„°ì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
        # (í–¥í›„ ì¶œë ¥ ì „ìš© ê·œì¹™ ì¶”ê°€ ê°€ëŠ¥)
        return self.check_input(text)

    def filter_text(self, text: str, replacement: str = "***") -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ê¸ˆì§€ì–´ë¥¼ ë§ˆìŠ¤í‚¹
        
        Args:
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            replacement: ëŒ€ì²´ ë¬¸ì
            
        Returns:
            í•„í„°ë§ëœ í…ìŠ¤íŠ¸
        """
        result = text
        detected_words = []

        # ëª¨ë“  íŒ¨í„´ì„ ì°¾ì•„ì„œ ë§ˆìŠ¤í‚¹
        all_patterns = []
        for patterns in self.harmful_patterns.values():
            all_patterns.extend(patterns)
        all_patterns.extend(self.spam_patterns)

        for pattern in all_patterns:
            matches = re.findall(pattern, result, re.IGNORECASE)
            if matches:
                detected_words.extend(matches)
                # íŒ¨í„´ì„ ëŒ€ì²´ ë¬¸ìë¡œ ë³€ê²½
                result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)

        # ì»¤ìŠ¤í…€ ë¸”ë™ë¦¬ìŠ¤íŠ¸ë„ ë§ˆìŠ¤í‚¹
        for word in self.custom_blacklist:
            if word.lower() in result.lower():
                result = re.sub(
                    re.escape(word),
                    replacement,
                    result,
                    flags=re.IGNORECASE
                )

        return result
    
    def classify_category(self, title: str, content: str) -> str:
        """
        ê²Œì‹œê¸€ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ (LLM í™œìš©)
        
        Args:
            title: ê²Œì‹œê¸€ ì œëª©
            content: ê²Œì‹œê¸€ ë‚´ìš©
            
        Returns:
            str: ì¹´í…Œê³ ë¦¬ëª… (ì˜ˆ: "ì¶•êµ¬ë¶„ì„", "ììœ ê²Œì‹œíŒ", "ì§ˆë¬¸", "ì •ë³´ê³µìœ " ë“±)
        """
        if not self.llm_client:
            return "general"  # LLM ë¹„í™œì„±í™” ì‹œ ê¸°ë³¸ê°’
        
        try:
            prompt = f"""ë‹¤ìŒ ê²Œì‹œê¸€ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ìë™ìœ¼ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”.

ì œëª©: "{title}"
ë‚´ìš©: "{content[:500]}"  # ë‚´ìš©ì´ ê¸¸ë©´ 500ìê¹Œì§€ë§Œ

ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:
- ì¶•êµ¬ë¶„ì„: ê²½ê¸° ë¶„ì„, ì „ìˆ  ë¶„ì„, íŒ€/ì„ ìˆ˜ ë¶„ì„
- ììœ ê²Œì‹œíŒ: ì¼ë°˜ì ì¸ ëŒ€í™”, ììœ ë¡œìš´ ì£¼ì œ
- ì§ˆë¬¸: ì§ˆë¬¸, ë„ì›€ ìš”ì²­
- ì •ë³´ê³µìœ : ë‰´ìŠ¤, ì •ë³´ ê³µìœ , ë§í¬ ê³µìœ 
- í›„ê¸°: ê²½ê¸° í›„ê¸°, ê´€ëŒ í›„ê¸°
- general: ê¸°íƒ€

ì¹´í…Œê³ ë¦¬ëª…ë§Œ ë°˜í™˜í•´ì£¼ì„¸ìš” (ì˜ˆ: "ì¶•êµ¬ë¶„ì„", "ììœ ê²Œì‹œíŒ" ë“±)."""

            response = self.llm_client.chat.completions.create(
                model=self.llm_model,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê²Œì‹œê¸€ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•˜ê²Œ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=50,
            )
            
            category = response.choices[0].message.content.strip()
            
            # ë”°ì˜´í‘œ ì œê±°
            category = category.strip('"\'')
            
            # ìœ íš¨í•œ ì¹´í…Œê³ ë¦¬ í™•ì¸
            valid_categories = ["ì¶•êµ¬ë¶„ì„", "ììœ ê²Œì‹œíŒ", "ì§ˆë¬¸", "ì •ë³´ê³µìœ ", "í›„ê¸°", "general"]
            if category not in valid_categories:
                logger.warning(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ì¹´í…Œê³ ë¦¬: {category}, ê¸°ë³¸ê°’ ì‚¬ìš©")
                return "general"
            
            logger.info(f"ğŸ“‚ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜: {category}")
            return category
            
        except Exception as e:
            logger.warning(f"âš ï¸ ì¹´í…Œê³ ë¦¬ ìë™ ë¶„ë¥˜ ì‹¤íŒ¨: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©")
            return "general"


