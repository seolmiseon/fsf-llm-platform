"""
OpenAI API í´ë¼ì´ì–¸íŠ¸ ë° í—¬í¼ í•¨ìˆ˜
"""
import os
import logging
from typing import Optional, List, Dict, Any
from openai import OpenAI, APIError, RateLimitError
import tiktoken

logger = logging.getLogger(__name__)

class OpenAIService:
    """OpenAI APIì™€ ìƒí˜¸ì‘ìš©í•˜ëŠ” ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        """
        OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        
        ğŸ“– ê³µì‹ ë¬¸ì„œ: https://platform.openai.com/docs/guides/chat-completions
        """
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        self.client = OpenAI(api_key=api_key)
        self.chat_model = os.getenv("OPENAI_CHAT_MODEL", "gpt-4o-mini")
        self.embedding_model = os.getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")
        
        logger.info(f"âœ… OpenAI ì´ˆê¸°í™” ì™„ë£Œ: {self.chat_model}")
    
    def chat(
        self,
        messages: List[Dict[str, str]],
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        timeout: int = 30
    ) -> str:
        """
        Chat Completion API í˜¸ì¶œ
        
        ğŸ“– ê³µì‹ ë¬¸ì„œ: https://platform.openai.com/docs/api-reference/chat/create
        ğŸ”— ì—”ë“œí¬ì¸íŠ¸: POST /chat/completions
        
        Args:
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ [{"role": "user", "content": "..."}]
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)
            temperature: ì°½ì˜ì„± (0.0~2.0, ê¸°ë³¸ê°’ 0.7)
            max_tokens: ìµœëŒ€ í† í° (None=ë¬´ì œí•œ)
            timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
        
        Returns:
            ëª¨ë¸ì˜ ì‘ë‹µ í…ìŠ¤íŠ¸
        
        Raises:
            RateLimitError: API ì†ë„ ì œí•œ ì´ˆê³¼
            APIError: API ì—ëŸ¬
        
        Example:
            >>> service = OpenAIService()
            >>> response = service.chat(
            ...     messages=[{"role": "user", "content": "ì†í¥ë¯¼ ìµœê·¼ í¼?"}],
            ...     system_prompt="ì¶•êµ¬ ì „ë¬¸ê°€"
            ... )
        """
        try:
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ë©”ì‹œì§€ ì•ì— ì¶”ê°€
            if system_prompt:
                messages = [
                    {"role": "system", "content": system_prompt},
                    *messages
                ]
            
            # API í˜¸ì¶œ
            response = self.client.chat.completions.create(
                model=self.chat_model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=timeout
            )
            
            # ì‘ë‹µ ì¶”ì¶œ
            content = response.choices[0].message.content
            
            # í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…
            logger.info(
                f"ğŸ“Š í† í° ì‚¬ìš©: "
                f"ì…ë ¥={response.usage.prompt_tokens} "
                f"ì¶œë ¥={response.usage.completion_tokens} "
                f"í•©ê³„={response.usage.total_tokens}"
            )
            
            return content
        
        except RateLimitError as e:
            logger.error(f"âš ï¸ API ì†ë„ ì œí•œ: {e}")
            raise
        except APIError as e:
            logger.error(f"âŒ OpenAI API ì—ëŸ¬: {e}")
            raise
    
    def embedding(self, text: str) -> List[float]:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        
        ğŸ“– ê³µì‹ ë¬¸ì„œ: https://platform.openai.com/docs/api-reference/embeddings
        ğŸ”— ì—”ë“œí¬ì¸íŠ¸: POST /embeddings
        
        Args:
            text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
        
        Returns:
            ì„ë² ë”© ë²¡í„° (1536ì°¨ì›)
        
        Example:
            >>> service = OpenAIService()
            >>> embedding = service.embedding("ì†í¥ë¯¼ ê³¨")
            >>> len(embedding)
            1536
        """
        try:
            response = self.client.embeddings.create(
                model=self.embedding_model,
                input=text
            )
            
            embedding = response.data[0].embedding
            logger.debug(f"âœ… ì„ë² ë”© ìƒì„±: {len(embedding)}ì°¨ì›")
            
            return embedding
        
        except APIError as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def embeddings_batch(self, texts: List[str]) -> List[List[float]]:
        """
        ì—¬ëŸ¬ í…ìŠ¤íŠ¸ì˜ ì„ë² ë”© ì¼ê´„ ìƒì„±
        
        Args:
            texts: í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ (ìµœëŒ€ 2048ê°œ)
        
        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        
        Raises:
            ValueError: í…ìŠ¤íŠ¸ ê°œìˆ˜ ì´ˆê³¼
        
        Example:
            >>> service = OpenAIService()
            >>> texts = ["ì†í¥ë¯¼", "ì¼€ì¸", "í™€ë€ë“œ"]
            >>> embeddings = service.embeddings_batch(texts)
        """
        if len(texts) > 2048:
            raise ValueError("ìµœëŒ€ 2048ê°œ í…ìŠ¤íŠ¸ê¹Œì§€ë§Œ ì²˜ë¦¬ ê°€ëŠ¥")
        
        try:
            response = self.client.embeddings.create(
                model=self.embedding_model,
                input=texts
            )
            
            # ì‘ë‹µì—ì„œ ì„ë² ë”© ì¶”ì¶œ (ì •ë ¬ë¨)
            embeddings = [item.embedding for item in response.data]
            logger.info(f"âœ… {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
            
            return embeddings
        
        except APIError as e:
            logger.error(f"âŒ ì¼ê´„ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            raise
    
    def count_tokens(self, text: str) -> int:
        """
        í…ìŠ¤íŠ¸ì˜ í† í° ê°œìˆ˜ ê³„ì‚°
        
        Args:
            text: ê³„ì‚°í•  í…ìŠ¤íŠ¸
        
        Returns:
            í† í° ê°œìˆ˜
        
        Example:
            >>> service = OpenAIService()
            >>> count = service.count_tokens("ì†í¥ë¯¼ì€ ì¢‹ì€ ì„ ìˆ˜ì…ë‹ˆë‹¤")
            >>> print(count)
            8
        """
        try:
            encoding = tiktoken.get_encoding("cl100k_base")
            tokens = encoding.encode(text)
            return len(tokens)
        except Exception as e:
            logger.warning(f"âš ï¸ í† í° ê³„ì‚° ì‹¤íŒ¨, ì¶”ì •ê°’ ì‚¬ìš©: {e}")
            # í´ë°±: ëŒ€ëµ 1ë‹¨ì–´ = 1.3 í† í°
            return int(len(text.split()) * 1.3)
    
    def estimate_cost(
        self,
        input_tokens: int,
        output_tokens: int
    ) -> Dict[str, Any]:
        """
        ì˜ˆìƒ ë¹„ìš© ê³„ì‚°
        
        ğŸ“– ê°€ê²©: https://openai.com/pricing
        - gpt-4o-mini input: $0.150 / 1M tokens
        - gpt-4o-mini output: $0.600 / 1M tokens
        - text-embedding-3-small: $0.020 / 1M tokens
        
        Args:
            input_tokens: ì…ë ¥ í† í° ê°œìˆ˜
            output_tokens: ì¶œë ¥ í† í° ê°œìˆ˜
        
        Returns:
            ë¹„ìš© ì •ë³´ ë”•ì…”ë„ˆë¦¬
        
        Example:
            >>> service = OpenAIService()
            >>> cost = service.estimate_cost(1000, 500)
            >>> print(f"${cost['total_usd']:.4f}")
        """
        # gpt-4o-mini ê°€ê²©
        input_cost = (input_tokens / 1_000_000) * 0.150
        output_cost = (output_tokens / 1_000_000) * 0.600
        total_cost = input_cost + output_cost
        
        return {
            "model": self.chat_model,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": input_tokens + output_tokens,
            "input_cost_usd": round(input_cost, 6),
            "output_cost_usd": round(output_cost, 6),
            "total_usd": round(total_cost, 6)
        }

    def validate_context_length(
        self,
        messages: List[Dict[str, str]],
        context: str = ""
    ) -> bool:
        """
        ë©”ì‹œì§€ì™€ ì»¨í…ìŠ¤íŠ¸ê°€ ëª¨ë¸ì˜ í† í° ì œí•œì„ ì´ˆê³¼í•˜ëŠ”ì§€ í™•ì¸
        
        gpt-4o-mini ì œí•œ: 128,000 í† í° (ì•ˆì „ì„ : 90%)
        
        Args:
            messages: ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
            context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸
        
        Returns:
            True if OK, False if ì´ˆê³¼
        """
        total_text = "\n".join([msg.get("content", "") for msg in messages]) + context
        token_count = self.count_tokens(total_text)
        max_tokens = 128_000 * 0.9  # ì•ˆì „ì„  90%
        
        if token_count > max_tokens:
            logger.warning(f"âš ï¸ í† í° ì´ˆê³¼: {token_count} / {int(max_tokens)}")
            return False
        
        return True