from fastapi import APIRouter, HTTPException
from typing import Optional
import re
import logging
from datetime import datetime

from ..models import ChatRequest, ChatResponse, ErrorResponse
from ..services.openai_service import OpenAIService
from ..services.rag_service import RAGService
from ..services.cache_service import CacheService  # â† ğŸ†• ì¶”ê°€!
from ..prompts.chat_prompts import SYSTEM_PROMPT, format_chat_context
from ..routers.stats import get_player_stats

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chat", tags=["AI Chat"])

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
openai_service = OpenAIService()
rag_service = RAGService()
cache_service = CacheService()  # â† ğŸ†• ì¶”ê°€!


def _is_stats_question(query: str) -> bool:
    """
    ë“ì /ì–´ì‹œìŠ¤íŠ¸ ë“± í†µê³„ì„± ì§ˆë¬¸ì¸ì§€ ê°„ë‹¨íˆ ê°ì§€
    (1ì°¨ ë²„ì „: í‚¤ì›Œë“œ ê¸°ë°˜)
    """
    stats_keywords = [
        "ë“ì ",
        "ê³¨",
        "ë„ì›€",
        "ì–´ì‹œìŠ¤íŠ¸",
        "assist",
        "assists",
        "goals",
        "scorer",
        "top scorer",
    ]
    q = query.lower()
    return any(kw in q or kw in query for kw in stats_keywords)


def _extract_english_name(query: str) -> Optional[str]:
    """
    ì§ˆë¬¸ì—ì„œ ì˜ë¬¸ ì„ ìˆ˜ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: "Son Heung-Min")
    - í•œê¸€ ì´ë¦„ ë§¤í•‘ì€ ì¶”í›„ í™•ì¥ (í˜„ì¬ëŠ” ì˜ë¬¸ ì´ë¦„ì´ í¬í•¨ëœ ê²½ìš°ë§Œ ì²˜ë¦¬)
    """
    # ë‘ ë‹¨ì–´ ì´ìƒ ì—°ì†ëœ ì˜ë¬¸ íŒ¨í„´
    matches = re.findall(r"[A-Za-z]+(?:\s+[A-Za-z]+)+", query)
    if not matches:
        return None
    # ê°€ì¥ ì²˜ìŒ ë§¤ì¹­ëœ ì´ë¦„ ì‚¬ìš©
    return matches[0].strip()


async def _build_stats_context(query: str) -> Optional[str]:
    """
    ìŠ¤íƒ¯ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œ, ì„ ìˆ˜ í†µê³„ APIë¥¼ í˜¸ì¶œí•´ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
    - 1ì°¨ ë²„ì „: ì˜ë¬¸ ì„ ìˆ˜ ì´ë¦„ + ê°œì¸ ìŠ¤íƒ¯ë§Œ ì§€ì›
    """
    if not _is_stats_question(query):
        return None

    player_name_en = _extract_english_name(query)
    if not player_name_en:
        # ì•„ì§ í•œê¸€ ì´ë¦„ â†’ ì˜ë¬¸ ë§¤í•‘ì€ ë¯¸êµ¬í˜„
        return None

    try:
        # ê¸°ì¡´ stats ë¼ìš°í„°ì˜ ë¡œì§ì„ ê·¸ëŒ€ë¡œ ì¬ì‚¬ìš©
        stats_response = await get_player_stats(player_name_en)
    except HTTPException:
        return None

    if not stats_response.get("success"):
        return None

    # stats_response êµ¬ì¡°ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    goals = stats_response.get("goals", 0)
    assists = stats_response.get("assists", 0)
    matches = stats_response.get("matches", 0)
    team = stats_response.get("team", "Unknown")

    return (
        f"[ì‹¤ì‹œê°„ ì„ ìˆ˜ í†µê³„]\n"
        f"ì„ ìˆ˜: {stats_response.get('name', player_name_en)}\n"
        f"íŒ€: {team}\n"
        f"ê²½ê¸° ìˆ˜: {matches}\n"
        f"ë“ì : {goals}ê³¨\n"
        f"ë„ì›€: {assists}ê°œ\n"
    )


@router.post(
    "",
    response_model=ChatResponse,
    responses={
        200: {"description": "ì±—ë´‡ ì‘ë‹µ ì„±ê³µ"},
        400: {"model": ErrorResponse, "description": "ì˜ëª»ëœ ìš”ì²­"},
        500: {"model": ErrorResponse, "description": "ì„œë²„ ì˜¤ë¥˜"},
    },
)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    AI ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (ìºì‹± ìµœì í™”)

    RAG + OpenAI + ìºì‹±ì„ í†µí•œ ë¹„ìš© ìµœì í™”

    Args:
        request: ChatRequest
            - query: ì‚¬ìš©ì ì§ˆë¬¸
            - top_k: RAG ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            - context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

    Returns:
        ChatResponse: AI ë‹µë³€ + ìºì‹œ ì •ë³´

    Example:
        >>> curl -X POST http://localhost:8080/api/llm/chat \\
        ...   -H "Content-Type: application/json" \\
        ...   -d '{"query": "ì†í¥ë¯¼ ìµœê·¼ í¼ì€?", "top_k": 5}'

        {
            "answer": "ì†í¥ë¯¼ì€ ìµœê·¼ 5ê²½ê¸°ì—ì„œ...",
            "sources": [],
            "tokens_used": 0,
            "confidence": 0.95,
            "cache_hit": true,
            "cache_source": "chromadb",
            "cost_saved": 0.001
        }
    """
    try:
        logger.info(f"ğŸ’¬ ì±—ë´‡ ìš”ì²­: {request.query}")

        # ============================================
        # âœ… STEP 1: ChromaDB ìºì‹œ í™•ì¸ ($0)
        # ============================================
        logger.debug("Step 1ï¸âƒ£: ChromaDB ìºì‹œ ê²€ìƒ‰ ì¤‘...")
        cached_answer = await cache_service.get_cached_answer(request.query)

        if cached_answer:
            logger.info(f"ğŸ¯ ìºì‹œëœ ë‹µë³€ ë°˜í™˜ (ë¹„ìš© $0)")
            return ChatResponse(
                answer=cached_answer["answer"],
                sources=[],  # ìºì‹œ ë‹µë³€ì€ ì†ŒìŠ¤ ì—†ìŒ
                tokens_used=0,  # ìºì‹œ íˆíŠ¸ = í† í° 0
                confidence=cached_answer["confidence"],
                cache_hit=True,  # â† ğŸ†•
                cache_source="chromadb",  # â† ğŸ†•
                cost_saved=0.001,  # â† ğŸ†• ì˜ˆìƒ ì ˆê° ë¹„ìš©
            )

        logger.debug("âš ï¸ ìºì‹œ ë¯¸ìŠ¤ â†’ ìƒˆë¡œìš´ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬")

        # ============================================
        # âœ… STEP 2: (ì„ íƒ) ìŠ¤íƒ¯ API ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± ($0.001 ë¯¸ë§Œ, ë‚´ë¶€ í˜¸ì¶œ)
        # ============================================
        logger.debug("Step 2ï¸âƒ£: ìŠ¤íƒ¯/íŒ€ ê´€ë ¨ ì§ˆë¬¸ì¸ì§€ í™•ì¸ ì¤‘...")
        stats_context = await _build_stats_context(request.query)

        # ============================================
        # âœ… STEP 3: RAG ê²€ìƒ‰ ($0)
        # ============================================
        logger.debug("Step 3ï¸âƒ£: RAG ê²€ìƒ‰ ì¤‘...")
        search_query = request.query
        rag_results = rag_service.search(
            collection_name="default", query=search_query, top_k=request.top_k
        )

        # RAG ê²°ê³¼ë¥¼ ì†ŒìŠ¤ë¡œ ë³€í™˜
        sources = [
            {
                "id": rag_results["ids"][i],
                "content": rag_results["documents"][i],
                "metadata": rag_results["metadatas"][i],
                "similarity": 1 - rag_results["distances"][i],
            }
            for i in range(len(rag_results["ids"]))
        ]

        logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì™„ë£Œ: {len(sources)}ê°œ ì†ŒìŠ¤")

        # ============================================
        # âœ… STEP 4: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (RAG + ì„ íƒì  ìŠ¤íƒ¯ ì»¨í…ìŠ¤íŠ¸) ($0)
        # ============================================
        logger.debug("Step 4ï¸âƒ£: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… ì¤‘...")
        rag_context_text = format_chat_context(sources)

        if stats_context:
            context_text = f"{stats_context}\n\n{rag_context_text}"
        else:
            context_text = rag_context_text

        # ============================================
        # âœ… STEP 5: OpenAI LLM í˜¸ì¶œ ($0.001) âš ï¸
        # ============================================
        logger.debug("Step 5ï¸âƒ£: OpenAI LLM í˜¸ì¶œ ì¤‘... (ë¹„ìš© ë°œìƒ!)")
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        user_message_with_context = f"""ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì‚¬ìš©ì ì§ˆë¬¸: {request.query}"""

        messages.append({"role": "user", "content": user_message_with_context})

        # LLM í˜¸ì¶œ
        ai_response = await openai_service.chat(messages=messages)

        # ============================================
        # âœ… STEP 6: ì‹¤ì œ í† í° ìˆ˜ ê³„ì‚° ($0)
        # ============================================
        logger.debug("Step 6ï¸âƒ£: í† í° ìˆ˜ ê³„ì‚° ì¤‘...")
        input_tokens = openai_service.count_tokens(user_message_with_context)
        output_tokens = openai_service.count_tokens(ai_response)
        total_tokens = input_tokens + output_tokens

        logger.info(
            f"ğŸ“Š í† í° ì‚¬ìš©: {total_tokens}ê°œ "
            f"(ì…ë ¥: {input_tokens}, ì¶œë ¥: {output_tokens})"
        )

        # ============================================
        # âœ… STEP 7: ChromaDBì— ë‹µë³€ ì €ì¥ ($0)
        # ============================================
        logger.debug("Step 7ï¸âƒ£: ChromaDBì— ë‹µë³€ ì €ì¥ ì¤‘...")
        cache_saved = await cache_service.cache_answer(
            query=request.query,
            answer=ai_response,
            metadata={
                "rag_sources": [s.get("id") for s in sources],
                "model": "gpt-4o-mini",
                "tokens": total_tokens,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
            },
        )

        if cache_saved:
            logger.info(f"âœ… ë‹µë³€ ìºì‹œ ì €ì¥ ì™„ë£Œ")
        else:
            logger.warning(f"âš ï¸ ë‹µë³€ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")

        logger.info(f"âœ… ì±—ë´‡ ì‘ë‹µ ìƒì„± & ìºì‹œ ì €ì¥ ì™„ë£Œ")

        # ============================================
        # âœ… STEP 8: ì‚¬ìš©ìì—ê²Œ ë°˜í™˜ âœ…
        # ============================================
        return ChatResponse(
            answer=ai_response,
            sources=[s.get("id", "") for s in sources],
            tokens_used=total_tokens,
            confidence=0.85,
            cache_hit=False,  # â† ğŸ†• ìºì‹œ ë¯¸ìŠ¤
            cache_source="llm",  # â† ğŸ†• LLMì—ì„œ ìƒì„±
            cost_saved=0.0,  # â† ğŸ†• ìºì‹œ ë¯¸ìŠ¤ì´ë¯€ë¡œ ë¹„ìš© ë°œìƒ
        )

    except Exception as e:
        logger.error(f"âŒ ì±—ë´‡ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì±—ë´‡ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")


@router.get("/health", response_model=dict, summary="ì±—ë´‡ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬")
async def chat_health():
    """ì±—ë´‡ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "service": "chat",
        "timestamp": datetime.now().isoformat(),
    }
