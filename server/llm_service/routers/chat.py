from fastapi import APIRouter, HTTPException
from typing import Optional, Dict
import re
import logging
import os
import json
from datetime import datetime

from ..models import ChatRequest, ChatResponse, ErrorResponse
from ..services.openai_service import OpenAIService
from ..services.rag_service import RAGService
from ..services.cache_service import CacheService  # â† ğŸ†• ì¶”ê°€!
from ..prompts.chat_prompts import SYSTEM_PROMPT, format_chat_context
from ..routers.stats import get_player_stats

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chat", tags=["AI Chat"])

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
openai_service = OpenAIService()
rag_service = RAGService()

# CacheService ì´ˆê¸°í™” (ChromaDB ì˜¤ë¥˜ ì‹œì—ë„ ì„œë²„ ê³„ì† ì‹¤í–‰)
try:
    cache_service = CacheService()
except Exception as e:
    logger.warning(f"âš ï¸ CacheService ì´ˆê¸°í™” ì‹¤íŒ¨ (ìºì‹œ ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
    cache_service = None

# í•œê¸€ ë§¤í•‘ í…Œì´ë¸” ì œê±°ë¨ - JSONì—ì„œ ko_name í•„ë“œë¡œ ì§ì ‘ ê²€ìƒ‰


def _is_stats_question(query: str) -> bool:
    """
    ë“ì /ì–´ì‹œìŠ¤íŠ¸/í¼ ë“± í†µê³„ì„± ì§ˆë¬¸ì¸ì§€ ê°„ë‹¨íˆ ê°ì§€
    (1ì°¨ ë²„ì „: í‚¤ì›Œë“œ ê¸°ë°˜)
    """
    stats_keywords = [
        "ë“ì ",
        "ê³¨",
        "ë„ì›€",
        "ì–´ì‹œìŠ¤íŠ¸",
        "í¼",
        "í†µê³„",
        "ì‹œì¦Œ",
        "assist",
        "assists",
        "goals",
        "scorer",
        "top scorer",
        "form",
        "stats",
        "statistics",
    ]
    q = query.lower()
    return any(kw in q or kw in query for kw in stats_keywords)


def _extract_english_name(query: str) -> Optional[str]:
    """
    ì§ˆë¬¸ì—ì„œ ì˜ë¬¸ ì„ ìˆ˜ ì´ë¦„ ì¶”ì¶œ
    - ì˜ë¬¸ ì´ë¦„ì´ ì§ì ‘ í¬í•¨ëœ ê²½ìš°: ê·¸ëŒ€ë¡œ ë°˜í™˜
    - í•œê¸€ ì´ë¦„ì€ _build_stats_context()ì—ì„œ ì§ì ‘ ì²˜ë¦¬ (JSONì˜ ko_name í•„ë“œë¡œ ê²€ìƒ‰)
    """
    # ì˜ë¬¸ ì´ë¦„ íŒ¨í„´ í™•ì¸ (ë‘ ë‹¨ì–´ ì´ìƒ)
    matches = re.findall(r"[A-Za-z]+(?:\s+[A-Za-z]+)+", query)
    if matches:
        return matches[0].strip()
    
    return None


async def _build_stats_context(query: str) -> Optional[str]:
    """
    ìŠ¤íƒ¯ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œ, ì„ ìˆ˜ í†µê³„ APIë¥¼ í˜¸ì¶œí•´ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
    - JSON ìºì‹œì—ì„œë§Œ ê°€ì ¸ì˜´ (ìŠ¤í¬ë˜í•‘ ì—†ìŒ)
    - í•œê¸€ ì´ë¦„ë„ ì§ì ‘ ì§€ì›
    """
    if not _is_stats_question(query):
        return None

    # 1. ì˜ë¬¸ ì´ë¦„ ì¶”ì¶œ ì‹œë„
    player_name = _extract_english_name(query)
    
    # 2. ì˜ë¬¸ ì´ë¦„ì´ ì—†ìœ¼ë©´ í•œê¸€ ì´ë¦„ ì¶”ì¶œ ì‹œë„
    if not player_name:
        korean_matches = re.findall(r"[ê°€-í£]{2,4}", query)
        if korean_matches:
            player_name = korean_matches[0]  # ì²« ë²ˆì§¸ í•œê¸€ ì´ë¦„ ì‚¬ìš©
    
    if not player_name:
        return None

    try:
        # JSON ìºì‹œì—ì„œ í†µê³„ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ì—†ìŒ, í•œê¸€/ì˜ë¬¸ ëª¨ë‘ ì§€ì›)
        stats_response = await get_player_stats(player_name)
    except HTTPException:
        return None

    if not stats_response.get("success"):
        return None

    # stats_response êµ¬ì¡°ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    goals = stats_response.get("goals", 0)
    assists = stats_response.get("assists", 0)
    matches = stats_response.get("matches", 0)
    team = stats_response.get("team", "Unknown")

    return (
        f"[ì„ ìˆ˜ í†µê³„ (JSON ìºì‹œ)]\n"
        f"ì„ ìˆ˜: {stats_response.get('name', player_name)}\n"
        f"íŒ€: {team}\n"
        f"ê²½ê¸° ìˆ˜: {matches}\n"
        f"ë“ì : {goals}ê³¨\n"
        f"ë„ì›€: {assists}ê°œ\n"
    )


@router.post(
    "",
    response_model=ChatResponse,
    responses={
        200: {"description": "ì±—ë´‡ ì‘ë‹µ ì„±ê³µ"},
        400: {"model": ErrorResponse, "description": "ì˜ëª»ëœ ìš”ì²­"},
        500: {"model": ErrorResponse, "description": "ì„œë²„ ì˜¤ë¥˜"},
    },
)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    AI ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (ìºì‹± ìµœì í™”)

    RAG + OpenAI + ìºì‹±ì„ í†µí•œ ë¹„ìš© ìµœì í™”

    Args:
        request: ChatRequest
            - query: ì‚¬ìš©ì ì§ˆë¬¸
            - top_k: RAG ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            - context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

    Returns:
        ChatResponse: AI ë‹µë³€ + ìºì‹œ ì •ë³´

    Example:
        >>> curl -X POST http://localhost:8080/api/llm/chat \\
        ...   -H "Content-Type: application/json" \\
        ...   -d '{"query": "ì†í¥ë¯¼ ìµœê·¼ í¼ì€?", "top_k": 5}'

        {
            "answer": "ì†í¥ë¯¼ì€ ìµœê·¼ 5ê²½ê¸°ì—ì„œ...",
            "sources": [],
            "tokens_used": 0,
            "confidence": 0.95,
            "cache_hit": true,
            "cache_source": "chromadb",
            "cost_saved": 0.001
        }
    """
    try:
        logger.info(f"ğŸ’¬ ì±—ë´‡ ìš”ì²­: {request.query}")

        # ============================================
        # âœ… STEP 1: ChromaDB ìºì‹œ í™•ì¸ ($0) - í†µê³„ ì§ˆë¬¸ì´ ì•„ë‹ ë•Œë§Œ
        # ============================================
        is_stats_q = _is_stats_question(request.query)
        cached_answer = None
        
        # í†µê³„ ì§ˆë¬¸ì´ë©´ ìºì‹œ ìŠ¤í‚µ (RAG ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ì •ë³´ ì°¾ê¸°)
        if not is_stats_q:
            logger.debug("Step 1ï¸âƒ£: ChromaDB ìºì‹œ ê²€ìƒ‰ ì¤‘...")
            if cache_service:
                cached_answer = await cache_service.get_cached_answer(request.query)

            if cached_answer:
                logger.info(f"ğŸ¯ ìºì‹œëœ ë‹µë³€ ë°˜í™˜ (ë¹„ìš© $0)")
                return ChatResponse(
                    answer=cached_answer["answer"],
                    sources=[],  # ìºì‹œ ë‹µë³€ì€ ì†ŒìŠ¤ ì—†ìŒ
                    tokens_used=0,  # ìºì‹œ íˆíŠ¸ = í† í° 0
                    confidence=cached_answer["confidence"],
                    cache_hit=True,
                    cache_source="chromadb",
                    cost_saved=0.001,
                )

        # ============================================
        # âœ… STEP 2: í†µê³„ ì§ˆë¬¸ì¸ ê²½ìš° JSON ìºì‹œì—ì„œ í†µê³„ ê°€ì ¸ì˜¤ê¸°
        # ============================================
        stats_context = None
        if is_stats_q:
            logger.info("ğŸ“Š í†µê³„ ì§ˆë¬¸ ê°ì§€ â†’ JSON ìºì‹œì—ì„œ í†µê³„ í™•ì¸ ì¤‘...")
            stats_context = await _build_stats_context(request.query)
            if stats_context:
                logger.info("âœ… JSON ìºì‹œì—ì„œ í†µê³„ ë°ì´í„° í™•ì¸")
            else:
                logger.debug("âš ï¸ JSON ìºì‹œì— í†µê³„ ë°ì´í„° ì—†ìŒ â†’ RAG ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬")

        logger.debug("âš ï¸ ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” í†µê³„ ì§ˆë¬¸ â†’ RAG ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬")

        # ============================================
        # âœ… STEP 3: RAG ê²€ìƒ‰ ($0) - ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰
        # ============================================
        logger.debug("Step 3ï¸âƒ£: RAG ê²€ìƒ‰ ì¤‘... (í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš©)")
        search_query = request.query
        rag_results = rag_service.search(
            collection_name="default", query=search_query, top_k=request.top_k
        )

        # RAG ê²°ê³¼ë¥¼ ì†ŒìŠ¤ë¡œ ë³€í™˜
        sources = [
            {
                "id": rag_results["ids"][i],
                "content": rag_results["documents"][i],
                "metadata": rag_results["metadatas"][i],
                "similarity": 1 - rag_results["distances"][i],
            }
            for i in range(len(rag_results["ids"]))
        ]

        logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì™„ë£Œ: {len(sources)}ê°œ ì†ŒìŠ¤")

        # ============================================
        # âœ… STEP 4: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (RAG + ì„ íƒì  ìŠ¤íƒ¯ ì»¨í…ìŠ¤íŠ¸) ($0)
        # ============================================
        logger.debug("Step 4ï¸âƒ£: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… ì¤‘...")
        rag_context_text = format_chat_context(sources)
        
        if stats_context:
            context_text = f"{stats_context}\n\n{rag_context_text}"
        else:
            context_text = rag_context_text

        # ============================================
        # âœ… STEP 5: OpenAI LLM í˜¸ì¶œ ($0.001) âš ï¸
        # ============================================
        logger.debug("Step 5ï¸âƒ£: OpenAI LLM í˜¸ì¶œ ì¤‘... (ë¹„ìš© ë°œìƒ!)")
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        user_message_with_context = f"""ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì‚¬ìš©ì ì§ˆë¬¸: {request.query}"""

        messages.append({"role": "user", "content": user_message_with_context})

        # LLM í˜¸ì¶œ
        ai_response = await openai_service.chat(messages=messages)

        # ============================================
        # âœ… STEP 6: ì‹¤ì œ í† í° ìˆ˜ ê³„ì‚° ($0)
        # ============================================
        logger.debug("Step 6ï¸âƒ£: í† í° ìˆ˜ ê³„ì‚° ì¤‘...")
        input_tokens = openai_service.count_tokens(user_message_with_context)
        output_tokens = openai_service.count_tokens(ai_response)
        total_tokens = input_tokens + output_tokens

        logger.info(
            f"ğŸ“Š í† í° ì‚¬ìš©: {total_tokens}ê°œ "
            f"(ì…ë ¥: {input_tokens}, ì¶œë ¥: {output_tokens})"
        )

        # ============================================
        # âœ… STEP 7: ChromaDBì— ë‹µë³€ ì €ì¥ ($0)
        # ============================================
        logger.debug("Step 7ï¸âƒ£: ChromaDBì— ë‹µë³€ ì €ì¥ ì¤‘...")
        cache_saved = False
        if cache_service:
            cache_saved = await cache_service.cache_answer(
                query=request.query,
                answer=ai_response,
                metadata={
                    "rag_sources": [s.get("id") for s in sources],
                    "model": "gpt-4o-mini",
                    "tokens": total_tokens,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                },
            )

            if cache_saved:
                logger.info(f"âœ… ë‹µë³€ ìºì‹œ ì €ì¥ ì™„ë£Œ")
            else:
                logger.warning(f"âš ï¸ ë‹µë³€ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")

        logger.info(f"âœ… ì±—ë´‡ ì‘ë‹µ ìƒì„± & ìºì‹œ ì €ì¥ ì™„ë£Œ")

        # ============================================
        # âœ… STEP 8: ì‚¬ìš©ìì—ê²Œ ë°˜í™˜ âœ…
        # ============================================
        return ChatResponse(
            answer=ai_response,
            sources=[s.get("id", "") for s in sources],
            tokens_used=total_tokens,
            confidence=0.85,
            cache_hit=False,  # â† ğŸ†• ìºì‹œ ë¯¸ìŠ¤
            cache_source="llm",  # â† ğŸ†• LLMì—ì„œ ìƒì„±
            cost_saved=0.0,  # â† ğŸ†• ìºì‹œ ë¯¸ìŠ¤ì´ë¯€ë¡œ ë¹„ìš© ë°œìƒ
        )

    except Exception as e:
        logger.error(f"âŒ ì±—ë´‡ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì±—ë´‡ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")


@router.get("/health", response_model=dict, summary="ì±—ë´‡ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬")
async def chat_health():
    """ì±—ë´‡ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "service": "chat",
        "timestamp": datetime.now().isoformat(),
    }
