from fastapi import APIRouter, HTTPException
from typing import Optional
import logging
from datetime import datetime

from ..models import ChatRequest, ChatResponse, ErrorResponse
from ..services.openai_service import OpenAIService
from ..services.rag_service import RAGService
from ..services.cache_service import CacheService  # â† ğŸ†• ì¶”ê°€!
from ..prompts.chat_prompts import SYSTEM_PROMPT, format_chat_context

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chat", tags=["AI Chat"])

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
openai_service = OpenAIService()
rag_service = RAGService()
cache_service = CacheService()  # â† ğŸ†• ì¶”ê°€!


@router.post(
    "",
    response_model=ChatResponse,
    responses={
        200: {"description": "ì±—ë´‡ ì‘ë‹µ ì„±ê³µ"},
        400: {"model": ErrorResponse, "description": "ì˜ëª»ëœ ìš”ì²­"},
        500: {"model": ErrorResponse, "description": "ì„œë²„ ì˜¤ë¥˜"},
    },
)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    AI ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (ìºì‹± ìµœì í™”)

    RAG + OpenAI + ìºì‹±ì„ í†µí•œ ë¹„ìš© ìµœì í™”

    Args:
        request: ChatRequest
            - query: ì‚¬ìš©ì ì§ˆë¬¸
            - top_k: RAG ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            - context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

    Returns:
        ChatResponse: AI ë‹µë³€ + ìºì‹œ ì •ë³´

    Example:
        >>> curl -X POST http://localhost:8080/api/llm/chat \\
        ...   -H "Content-Type: application/json" \\
        ...   -d '{"query": "ì†í¥ë¯¼ ìµœê·¼ í¼ì€?", "top_k": 5}'

        {
            "answer": "ì†í¥ë¯¼ì€ ìµœê·¼ 5ê²½ê¸°ì—ì„œ...",
            "sources": [],
            "tokens_used": 0,
            "confidence": 0.95,
            "cache_hit": true,
            "cache_source": "chromadb",
            "cost_saved": 0.001
        }
    """
    try:
        logger.info(f"ğŸ’¬ ì±—ë´‡ ìš”ì²­: {request.query}")

        # ============================================
        # âœ… STEP 1: ChromaDB ìºì‹œ í™•ì¸ ($0)
        # ============================================
        logger.debug("Step 1ï¸âƒ£: ChromaDB ìºì‹œ ê²€ìƒ‰ ì¤‘...")
        cached_answer = await cache_service.get_cached_answer(request.query)

        if cached_answer:
            logger.info(f"ğŸ¯ ìºì‹œëœ ë‹µë³€ ë°˜í™˜ (ë¹„ìš© $0)")
            return ChatResponse(
                answer=cached_answer["answer"],
                sources=[],  # ìºì‹œ ë‹µë³€ì€ ì†ŒìŠ¤ ì—†ìŒ
                tokens_used=0,  # ìºì‹œ íˆíŠ¸ = í† í° 0
                confidence=cached_answer["confidence"],
                cache_hit=True,  # â† ğŸ†•
                cache_source="chromadb",  # â† ğŸ†•
                cost_saved=0.001,  # â† ğŸ†• ì˜ˆìƒ ì ˆê° ë¹„ìš©
            )

        logger.debug("âš ï¸ ìºì‹œ ë¯¸ìŠ¤ â†’ ìƒˆë¡œìš´ ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬")

        # ============================================
        # âœ… STEP 2: RAG ê²€ìƒ‰ ($0)
        # ============================================
        logger.debug("Step 2ï¸âƒ£: RAG ê²€ìƒ‰ ì¤‘...")
        search_query = request.query
        rag_results = rag_service.search(
            collection_name="default", query=search_query, top_k=request.top_k
        )

        # RAG ê²°ê³¼ë¥¼ ì†ŒìŠ¤ë¡œ ë³€í™˜
        sources = [
            {
                "id": rag_results["ids"][i],
                "content": rag_results["documents"][i],
                "metadata": rag_results["metadatas"][i],
                "similarity": 1 - rag_results["distances"][i],
            }
            for i in range(len(rag_results["ids"]))
        ]

        logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì™„ë£Œ: {len(sources)}ê°œ ì†ŒìŠ¤")

        # ============================================
        # âœ… STEP 3: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… ($0)
        # ============================================
        logger.debug("Step 3ï¸âƒ£: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… ì¤‘...")
        context_text = format_chat_context(sources)

        # ============================================
        # âœ… STEP 4: OpenAI LLM í˜¸ì¶œ ($0.001) âš ï¸
        # ============================================
        logger.debug("Step 4ï¸âƒ£: OpenAI LLM í˜¸ì¶œ ì¤‘... (ë¹„ìš© ë°œìƒ!)")
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        user_message_with_context = f"""ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì‚¬ìš©ì ì§ˆë¬¸: {request.query}"""

        messages.append({"role": "user", "content": user_message_with_context})

        # LLM í˜¸ì¶œ
        ai_response = openai_service.chat(messages=messages)

        # ============================================
        # âœ… STEP 5: ì‹¤ì œ í† í° ìˆ˜ ê³„ì‚° ($0)
        # ============================================
        logger.debug("Step 5ï¸âƒ£: í† í° ìˆ˜ ê³„ì‚° ì¤‘...")
        input_tokens = openai_service.count_tokens(user_message_with_context)
        output_tokens = openai_service.count_tokens(ai_response)
        total_tokens = input_tokens + output_tokens

        logger.info(
            f"ğŸ“Š í† í° ì‚¬ìš©: {total_tokens}ê°œ "
            f"(ì…ë ¥: {input_tokens}, ì¶œë ¥: {output_tokens})"
        )

        # ============================================
        # âœ… STEP 6: ChromaDBì— ë‹µë³€ ì €ì¥ ($0)
        # ============================================
        logger.debug("Step 6ï¸âƒ£: ChromaDBì— ë‹µë³€ ì €ì¥ ì¤‘...")
        cache_saved = await cache_service.cache_answer(
            query=request.query,
            answer=ai_response,
            metadata={
                "rag_sources": [s.get("id") for s in sources],
                "model": "gpt-4o-mini",
                "tokens": total_tokens,
                "input_tokens": input_tokens,
                "output_tokens": output_tokens,
            },
        )

        if cache_saved:
            logger.info(f"âœ… ë‹µë³€ ìºì‹œ ì €ì¥ ì™„ë£Œ")
        else:
            logger.warning(f"âš ï¸ ë‹µë³€ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")

        logger.info(f"âœ… ì±—ë´‡ ì‘ë‹µ ìƒì„± & ìºì‹œ ì €ì¥ ì™„ë£Œ")

        # ============================================
        # âœ… STEP 7: ì‚¬ìš©ìì—ê²Œ ë°˜í™˜ âœ…
        # ============================================
        return ChatResponse(
            answer=ai_response,
            sources=[s.get("id", "") for s in sources],
            tokens_used=total_tokens,
            confidence=0.85,
            cache_hit=False,  # â† ğŸ†• ìºì‹œ ë¯¸ìŠ¤
            cache_source="llm",  # â† ğŸ†• LLMì—ì„œ ìƒì„±
            cost_saved=0.0,  # â† ğŸ†• ìºì‹œ ë¯¸ìŠ¤ì´ë¯€ë¡œ ë¹„ìš© ë°œìƒ
        )

    except Exception as e:
        logger.error(f"âŒ ì±—ë´‡ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì±—ë´‡ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")


@router.get("/health", response_model=dict, summary="ì±—ë´‡ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬")
async def chat_health():
    """ì±—ë´‡ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "service": "chat",
        "timestamp": datetime.now().isoformat(),
    }
