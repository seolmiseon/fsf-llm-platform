from fastapi import APIRouter, HTTPException
from typing import Optional, Dict
import re
import logging
import os
import json
from datetime import datetime

from ..models import ChatRequest, ChatResponse, ErrorResponse
from ..services.openai_service import OpenAIService
from ..services.rag_service import RAGService
from ..services.cache_service import CacheService  # â† ğŸ†• ì¶”ê°€!
from ..services.content_safety_service import ContentSafetyService  # â† ğŸ†• ì½˜í…ì¸  í•„í„°ë§ ì¶”ê°€!
from ..prompts.chat_prompts import SYSTEM_PROMPT, format_chat_context
from ..routers.stats import get_player_stats
from ..utils.realtime_router import is_realtime_required, should_skip_cache  # â† ğŸ†• Router ì¶”ê°€
from ..utils.cache_judge import CacheJudge  # â† ğŸ†• Judge ì¶”ê°€

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chat", tags=["AI Chat"])

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
openai_service = OpenAIService()
rag_service = RAGService()

# CacheService ì´ˆê¸°í™” (ChromaDB ì˜¤ë¥˜ ì‹œì—ë„ ì„œë²„ ê³„ì† ì‹¤í–‰)
try:
    cache_service = CacheService()
except Exception as e:
    logger.warning(f"âš ï¸ CacheService ì´ˆê¸°í™” ì‹¤íŒ¨ (ìºì‹œ ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
    cache_service = None

# ContentSafetyService ì´ˆê¸°í™” (ì½˜í…ì¸  í•„í„°ë§)
try:
    content_safety_service = ContentSafetyService()
except Exception as e:
    logger.warning(f"âš ï¸ ContentSafetyService ì´ˆê¸°í™” ì‹¤íŒ¨ (í•„í„°ë§ ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
    content_safety_service = None

# CacheJudge ì´ˆê¸°í™” (ìºì‹œ ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨)
try:
    cache_judge = CacheJudge()
except Exception as e:
    logger.warning(f"âš ï¸ CacheJudge ì´ˆê¸°í™” ì‹¤íŒ¨ (Judge ê¸°ëŠ¥ ë¹„í™œì„±í™”): {e}")
    cache_judge = None

# í•œê¸€ ë§¤í•‘ í…Œì´ë¸” ì œê±°ë¨ - JSONì—ì„œ ko_name í•„ë“œë¡œ ì§ì ‘ ê²€ìƒ‰


def _is_stats_question(query: str) -> bool:
    """
    ë“ì /ì–´ì‹œìŠ¤íŠ¸/í¼ ë“± í†µê³„ì„± ì§ˆë¬¸ì¸ì§€ ê°„ë‹¨íˆ ê°ì§€
    (1ì°¨ ë²„ì „: í‚¤ì›Œë“œ ê¸°ë°˜)
    """
    stats_keywords = [
        "ë“ì ",
        "ê³¨",
        "ë„ì›€",
        "ì–´ì‹œìŠ¤íŠ¸",
        "í¼",
        "í†µê³„",
        "ì‹œì¦Œ",
        "assist",
        "assists",
        "goals",
        "scorer",
        "top scorer",
        "form",
        "stats",
        "statistics",
    ]
    q = query.lower()
    return any(kw in q or kw in query for kw in stats_keywords)


def _extract_english_name(query: str) -> Optional[str]:
    """
    ì§ˆë¬¸ì—ì„œ ì˜ë¬¸ ì„ ìˆ˜ ì´ë¦„ ì¶”ì¶œ
    - ì˜ë¬¸ ì´ë¦„ì´ ì§ì ‘ í¬í•¨ëœ ê²½ìš°: ê·¸ëŒ€ë¡œ ë°˜í™˜
    - í•œê¸€ ì´ë¦„ì€ _build_stats_context()ì—ì„œ ì§ì ‘ ì²˜ë¦¬ (JSONì˜ ko_name í•„ë“œë¡œ ê²€ìƒ‰)
    """
    # ì˜ë¬¸ ì´ë¦„ íŒ¨í„´ í™•ì¸ (ë‘ ë‹¨ì–´ ì´ìƒ)
    matches = re.findall(r"[A-Za-z]+(?:\s+[A-Za-z]+)+", query)
    if matches:
        return matches[0].strip()
    
    return None


async def _build_stats_context(query: str) -> Optional[str]:
    """
    ìŠ¤íƒ¯ ê´€ë ¨ ì§ˆë¬¸ì¼ ë•Œ, ì„ ìˆ˜ í†µê³„ APIë¥¼ í˜¸ì¶œí•´ ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
    - JSON ìºì‹œì—ì„œë§Œ ê°€ì ¸ì˜´ (ìŠ¤í¬ë˜í•‘ ì—†ìŒ)
    - í•œê¸€ ì´ë¦„ë„ ì§ì ‘ ì§€ì›
    """
    if not _is_stats_question(query):
        return None

    # 1. ì˜ë¬¸ ì´ë¦„ ì¶”ì¶œ ì‹œë„
    player_name = _extract_english_name(query)
    
    # 2. ì˜ë¬¸ ì´ë¦„ì´ ì—†ìœ¼ë©´ í•œê¸€ ì´ë¦„ ì¶”ì¶œ ì‹œë„
    if not player_name:
        korean_matches = re.findall(r"[ê°€-í£]{2,4}", query)
        if korean_matches:
            player_name = korean_matches[0]  # ì²« ë²ˆì§¸ í•œê¸€ ì´ë¦„ ì‚¬ìš©
    
    if not player_name:
        return None

    try:
        # JSON ìºì‹œì—ì„œ í†µê³„ ê°€ì ¸ì˜¤ê¸° (ìŠ¤í¬ë˜í•‘ ì—†ìŒ, í•œê¸€/ì˜ë¬¸ ëª¨ë‘ ì§€ì›)
        stats_response = await get_player_stats(player_name)
    except HTTPException:
        return None

    if not stats_response.get("success"):
        return None

    # stats_response êµ¬ì¡°ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    goals = stats_response.get("goals", 0)
    assists = stats_response.get("assists", 0)
    matches = stats_response.get("matches", 0)
    team = stats_response.get("team", "Unknown")

    return (
        f"[ì„ ìˆ˜ í†µê³„ (JSON ìºì‹œ)]\n"
        f"ì„ ìˆ˜: {stats_response.get('name', player_name)}\n"
        f"íŒ€: {team}\n"
        f"ê²½ê¸° ìˆ˜: {matches}\n"
        f"ë“ì : {goals}ê³¨\n"
        f"ë„ì›€: {assists}ê°œ\n"
    )


@router.post(
    "",
    response_model=ChatResponse,
    responses={
        200: {"description": "ì±—ë´‡ ì‘ë‹µ ì„±ê³µ"},
        400: {"model": ErrorResponse, "description": "ì˜ëª»ëœ ìš”ì²­"},
        500: {"model": ErrorResponse, "description": "ì„œë²„ ì˜¤ë¥˜"},
    },
)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    AI ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸ (ìºì‹± ìµœì í™”)

    RAG + OpenAI + ìºì‹±ì„ í†µí•œ ë¹„ìš© ìµœì í™”

    Args:
        request: ChatRequest
            - query: ì‚¬ìš©ì ì§ˆë¬¸
            - top_k: RAG ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            - context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

    Returns:
        ChatResponse: AI ë‹µë³€ + ìºì‹œ ì •ë³´

    Example:
        >>> curl -X POST http://localhost:8080/api/llm/chat \\
        ...   -H "Content-Type: application/json" \\
        ...   -d '{"query": "ì†í¥ë¯¼ ìµœê·¼ í¼ì€?", "top_k": 5}'

        {
            "answer": "ì†í¥ë¯¼ì€ ìµœê·¼ 5ê²½ê¸°ì—ì„œ...",
            "sources": [],
            "tokens_used": 0,
            "confidence": 0.95,
            "cache_hit": true,
            "cache_source": "chromadb",
            "cost_saved": 0.001
        }
    """
    try:
        logger.info(f"ğŸ’¬ ì±—ë´‡ ìš”ì²­: {request.query}")

        # ============================================
        # ğŸ›¡ï¸ STEP 0: ì…ë ¥ ê²Œì´íŠ¸ì›¨ì´ - ì‚¬ìš©ì ì¿¼ë¦¬ í•„í„°ë§
        # ============================================
        if content_safety_service:
            logger.debug("ğŸ›¡ï¸ ì…ë ¥ í•„í„°ë§ ì¤‘...")
            input_check = content_safety_service.check_input(request.query)
            
            if not input_check.is_safe:
                logger.warning(
                    f"ğŸš« ìœ í•´ ì½˜í…ì¸  ê°ì§€ (ì…ë ¥): "
                    f"ì¹´í…Œê³ ë¦¬={input_check.category}, "
                    f"ê°ì§€ëœ ë‹¨ì–´={input_check.detected_words}, "
                    f"ì´ìœ ={input_check.reason}"
                )
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "ë¶€ì ì ˆí•œ ë‚´ìš©ì´ í¬í•¨ëœ ìš”ì²­ì…ë‹ˆë‹¤.",
                        "error_code": "INAPPROPRIATE_CONTENT",
                        "category": input_check.category.value if input_check.category else None,
                        "reason": input_check.reason
                    }
                )
            logger.debug("âœ… ì…ë ¥ í•„í„°ë§ í†µê³¼")

        # ============================================
        # ğŸšª ì…êµ¬ (Semantic Router): ì‹¤ì‹œê°„ ì •ë³´ í•„ìš” ì—¬ë¶€ íŒë‹¨
        # ============================================
        # ì œë¯¼ì˜ ì œì•ˆ 1: Decision Tree (Router ë‹¨ê³„ ë¶„ë¦¬)
        realtime_status = is_realtime_required(request.query)
        is_stats_q = _is_stats_question(request.query)
        
        # ì‹¤ì‹œê°„ ì •ë³´ í•„ìˆ˜ë©´ ìºì‹œ ìŠ¤í‚µ (API í˜¸ì¶œ í•„ìˆ˜)
        if realtime_status == "realtime":
            logger.info("ğŸ”´ ì‹¤ì‹œê°„ ì •ë³´ í•„ìˆ˜ â†’ ìºì‹œ ìŠ¤í‚µ, API í˜¸ì¶œ í•„ìˆ˜")
            # ìºì‹œ ìŠ¤í‚µí•˜ê³  ë°”ë¡œ RAG ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰
        else:
            # ============================================
            # âœ… 1ì°¨ ê²€ë¬¸ì†Œ (Cache Lookup): ìœ ì‚¬ë„ 0.75 ì´ìƒ ìºì‹œ ì¡°íšŒ
            # ============================================
            cached_answer = None
            if not is_stats_q and cache_service:
                logger.debug("Step 1ï¸âƒ£: ChromaDB ìºì‹œ ê²€ìƒ‰ ì¤‘... (ìœ ì‚¬ë„ 0.75 ì´ìƒ)")
                cached_answer = await cache_service.get_cached_answer(request.query)

            if cached_answer:
                # ============================================
                # âš–ï¸ 2ì°¨ ê²€ë¬¸ì†Œ (The Judge): ìºì‹œ ë°ì´í„° ì¶©ë¶„ì„± íŒë‹¨ (í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™”)
                # ============================================
                # ì œë¯¼ì˜ ì œì•ˆ 1: Judge ë…¸ë“œì—ì„œ ìµœì¢… íŒë‹¨
                # í•˜ì´ë¸Œë¦¬ë“œ ìµœì í™”: ìœ ì‚¬ë„ì— ë”°ë¼ Judge í˜¸ì¶œ ì—¬ë¶€ ê²°ì •
                similarity = cached_answer.get("similarity", 0.0)
                
                # ìœ ì‚¬ë„ 0.9 ì´ìƒ: Judge ìŠ¤í‚µ (ë¹„ìš© ì ˆê°, ë°”ë¡œ ìºì‹œ ì‚¬ìš©)
                if similarity >= 0.9:
                    logger.info(f"âœ… ë†’ì€ ìœ ì‚¬ë„ ({similarity:.2f}) â†’ Judge ìŠ¤í‚µ, ìºì‹œ ì‚¬ìš© (ë¹„ìš© $0)")
                    return ChatResponse(
                        answer=cached_answer["answer"],
                        sources=[],
                        tokens_used=0,
                        confidence=cached_answer["confidence"],
                        cache_hit=True,
                        cache_source="chromadb",
                        cost_saved=0.001,
                    )
                
                # ìœ ì‚¬ë„ 0.7~0.9: Judge í˜¸ì¶œ (ë¹„ìš© ë°œìƒ, í•˜ì§€ë§Œ í•„ìš”í•  ë•Œë§Œ)
                elif similarity >= 0.7 and cache_judge:
                    logger.info(f"âš–ï¸ ì¤‘ê°„ ìœ ì‚¬ë„ ({similarity:.2f}) â†’ Judge í˜¸ì¶œ (ë¹„ìš© ë°œìƒ)")
                    judge_result, judge_reason = await cache_judge.judge(
                        query=request.query,
                        cached_answer=cached_answer["answer"],
                        cache_similarity=similarity
                    )
                    
                    if judge_result == "YES":
                        # Judgeê°€ YES â†’ ìºì‹œ ì‚¬ìš©
                        logger.info(f"âœ… Judge ìŠ¹ì¸: ìºì‹œ ì‚¬ìš© (ì´ìœ : {judge_reason})")
                        return ChatResponse(
                            answer=cached_answer["answer"],
                            sources=[],
                            tokens_used=0,
                            confidence=cached_answer["confidence"],
                            cache_hit=True,
                            cache_source="chromadb",
                            cost_saved=0.001,
                        )
                    elif judge_result == "CALL_API":
                        # ğŸ†• Judgeê°€ CALL_API â†’ ê°•ì œ API í˜¸ì¶œ (Hallucination ë°©ì§€)
                        logger.warning(f"ğŸ”´ Judge ê°•ì œ API í˜¸ì¶œ ìš”ì²­: {judge_reason}")
                        # ìºì‹œ ë¬´ì‹œí•˜ê³  RAG ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰
                    else:
                        # Judgeê°€ NO/UNCERTAIN â†’ API í˜¸ì¶œ
                        logger.info(f"âš ï¸ Judge ê±°ë¶€: API í˜¸ì¶œ í•„ìš” (íŒë‹¨: {judge_result}, ì´ìœ : {judge_reason})")
                        # ìºì‹œ ë¬´ì‹œí•˜ê³  RAG ê²€ìƒ‰ìœ¼ë¡œ ì§„í–‰
                else:
                    # ìœ ì‚¬ë„ 0.7 ë¯¸ë§Œ ë˜ëŠ” Judge ì—†ìŒ â†’ ìºì‹œ ì‚¬ìš© (ë‚®ì€ ìœ ì‚¬ë„ì§€ë§Œ ì¼ë‹¨ ì‚¬ìš©)
                    logger.info(f"ğŸ¯ ìºì‹œëœ ë‹µë³€ ë°˜í™˜ (ìœ ì‚¬ë„ {similarity:.2f}, Judge ìŠ¤í‚µ)")
                    return ChatResponse(
                        answer=cached_answer["answer"],
                        sources=[],
                        tokens_used=0,
                        confidence=cached_answer["confidence"],
                        cache_hit=True,
                        cache_source="chromadb",
                        cost_saved=0.001,
                    )

        # ============================================
        # âœ… STEP 2: í†µê³„ ì§ˆë¬¸ì¸ ê²½ìš° JSON ìºì‹œì—ì„œ í†µê³„ ê°€ì ¸ì˜¤ê¸°
        # ============================================
        stats_context = None
        if is_stats_q:
            logger.info("ğŸ“Š í†µê³„ ì§ˆë¬¸ ê°ì§€ â†’ JSON ìºì‹œì—ì„œ í†µê³„ í™•ì¸ ì¤‘...")
            stats_context = await _build_stats_context(request.query)
            if stats_context:
                logger.info("âœ… JSON ìºì‹œì—ì„œ í†µê³„ ë°ì´í„° í™•ì¸")
            else:
                logger.debug("âš ï¸ JSON ìºì‹œì— í†µê³„ ë°ì´í„° ì—†ìŒ â†’ RAG ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬")

        logger.debug("âš ï¸ ìºì‹œ ë¯¸ìŠ¤ ë˜ëŠ” í†µê³„ ì§ˆë¬¸ â†’ RAG ê²€ìƒ‰ìœ¼ë¡œ ì²˜ë¦¬")

        # ============================================
        # âœ… STEP 3: RAG ê²€ìƒ‰ ($0) - ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰
        # ============================================
        logger.debug("Step 3ï¸âƒ£: RAG ê²€ìƒ‰ ì¤‘... (í…ìŠ¤íŠ¸ ì„ë² ë”© ì‚¬ìš©)")
        search_query = request.query
        rag_results = rag_service.search(
            collection_name="default", query=search_query, top_k=request.top_k
        )

        # RAG ê²°ê³¼ë¥¼ ì†ŒìŠ¤ë¡œ ë³€í™˜
        sources = [
            {
                "id": rag_results["ids"][i],
                "content": rag_results["documents"][i],
                "metadata": rag_results["metadatas"][i],
                "similarity": 1 - rag_results["distances"][i],
            }
            for i in range(len(rag_results["ids"]))
        ]

        logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì™„ë£Œ: {len(sources)}ê°œ ì†ŒìŠ¤")

        # ============================================
        # âœ… STEP 4: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… (RAG + ì„ íƒì  ìŠ¤íƒ¯ ì»¨í…ìŠ¤íŠ¸) ($0)
        # ============================================
        logger.debug("Step 4ï¸âƒ£: ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ… ì¤‘...")
        rag_context_text = format_chat_context(sources)
        
        if stats_context:
            context_text = f"{stats_context}\n\n{rag_context_text}"
        else:
            context_text = rag_context_text

        # ============================================
        # âœ… STEP 5: OpenAI LLM í˜¸ì¶œ ($0.001) âš ï¸
        # ============================================
        logger.debug("Step 5ï¸âƒ£: OpenAI LLM í˜¸ì¶œ ì¤‘... (ë¹„ìš© ë°œìƒ!)")
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        user_message_with_context = f"""ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì‚¬ìš©ì ì§ˆë¬¸: {request.query}"""

        messages.append({"role": "user", "content": user_message_with_context})

        # LLM í˜¸ì¶œ
        ai_response = await openai_service.chat(messages=messages)

        # ============================================
        # âœ… STEP 6: ì‹¤ì œ í† í° ìˆ˜ ê³„ì‚° ($0)
        # ============================================
        logger.debug("Step 6ï¸âƒ£: í† í° ìˆ˜ ê³„ì‚° ì¤‘...")
        input_tokens = openai_service.count_tokens(user_message_with_context)
        output_tokens = openai_service.count_tokens(ai_response)
        total_tokens = input_tokens + output_tokens

        logger.info(
            f"ğŸ“Š í† í° ì‚¬ìš©: {total_tokens}ê°œ "
            f"(ì…ë ¥: {input_tokens}, ì¶œë ¥: {output_tokens})"
        )

        # ============================================
        # âœ… STEP 7: ChromaDBì— ë‹µë³€ ì €ì¥ ($0)
        # ============================================
        logger.debug("Step 7ï¸âƒ£: ChromaDBì— ë‹µë³€ ì €ì¥ ì¤‘...")
        cache_saved = False
        if cache_service:
            cache_saved = await cache_service.cache_answer(
                query=request.query,
                answer=ai_response,
                metadata={
                    "rag_sources": [s.get("id") for s in sources],
                    "model": "gpt-4o-mini",
                    "tokens": total_tokens,
                    "input_tokens": input_tokens,
                    "output_tokens": output_tokens,
                },
            )

            if cache_saved:
                logger.info(f"âœ… ë‹µë³€ ìºì‹œ ì €ì¥ ì™„ë£Œ")
            else:
                logger.warning(f"âš ï¸ ë‹µë³€ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰)")

        logger.info(f"âœ… ì±—ë´‡ ì‘ë‹µ ìƒì„± & ìºì‹œ ì €ì¥ ì™„ë£Œ")

        # ============================================
        # âœ… STEP 8: ì‚¬ìš©ìì—ê²Œ ë°˜í™˜ âœ…
        # ============================================
        return ChatResponse(
            answer=ai_response,
            sources=[s.get("id", "") for s in sources],
            tokens_used=total_tokens,
            confidence=0.85,
            cache_hit=False,  # â† ğŸ†• ìºì‹œ ë¯¸ìŠ¤
            cache_source="llm",  # â† ğŸ†• LLMì—ì„œ ìƒì„±
            cost_saved=0.0,  # â† ğŸ†• ìºì‹œ ë¯¸ìŠ¤ì´ë¯€ë¡œ ë¹„ìš© ë°œìƒ
        )

    except Exception as e:
        logger.error(f"âŒ ì±—ë´‡ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì±—ë´‡ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")


@router.get("/health", response_model=dict, summary="ì±—ë´‡ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬")
async def chat_health():
    """ì±—ë´‡ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "service": "chat",
        "timestamp": datetime.now().isoformat(),
    }
