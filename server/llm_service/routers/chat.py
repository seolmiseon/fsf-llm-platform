"""
AI ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸
POST /api/llm/chat
"""

from fastapi import APIRouter, HTTPException
from typing import Optional
import logging
from datetime import datetime

from ..models import ChatRequest, ChatResponse, ErrorResponse
from ..services.openai_service import OpenAIService
from ..services.rag_service import RAGService
from ..prompts.chat_prompts import SYSTEM_PROMPT, format_chat_context

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/chat", tags=["AI Chat"])

# ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
openai_service = OpenAIService()
rag_service = RAGService()


@router.post(
    "",
    response_model=ChatResponse,
    responses={
        200: {"description": "ì±—ë´‡ ì‘ë‹µ ì„±ê³µ"},
        400: {"model": ErrorResponse, "description": "ì˜ëª»ëœ ìš”ì²­"},
        500: {"model": ErrorResponse, "description": "ì„œë²„ ì˜¤ë¥˜"},
    },
)
async def chat(request: ChatRequest) -> ChatResponse:
    """
    AI ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸

    RAG ê²€ìƒ‰ â†’ OpenAI GPT â†’ ë‹µë³€

    Args:
        request: ChatRequest
            - query: ì‚¬ìš©ì ì§ˆë¬¸
            - top_k: RAG ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜
            - context: ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸

    Returns:
        ChatResponse: AI ë‹µë³€
    """
    try:
        logger.info(f"ğŸ’¬ ì±—ë´‡ ìš”ì²­: {request.query}")

        # 1ï¸âƒ£ RAG ê²€ìƒ‰ (ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘)
        search_query = request.query
        rag_results = rag_service.search(
            collection_name="default", query=search_query, top_k=request.top_k
        )

        # RAG ê²°ê³¼ë¥¼ ì†ŒìŠ¤ë¡œ ë³€í™˜
        sources = [
            {
                "id": rag_results["ids"][i],
                "content": rag_results["documents"][i],
                "metadata": rag_results["metadatas"][i],
                "similarity": 1 - rag_results["distances"][i],
            }
            for i in range(len(rag_results["ids"]))
        ]

        logger.info(f"ğŸ” RAG ê²€ìƒ‰ ì™„ë£Œ: {len(sources)}ê°œ ì†ŒìŠ¤")

        # 2ï¸âƒ£ ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context_text = format_chat_context(sources)

        # 3ï¸âƒ£ OpenAI í˜¸ì¶œ
        messages = [{"role": "system", "content": SYSTEM_PROMPT}]

        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€ (ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        user_message_with_context = f"""ì»¨í…ìŠ¤íŠ¸:
{context_text}

ì‚¬ìš©ì ì§ˆë¬¸: {request.query}"""

        messages.append({"role": "user", "content": user_message_with_context})

        # âœ… ì˜¬ë°”ë¥¸ ë©”ì„œë“œëª…: chat_completion() â†’ chat()
        ai_response = openai_service.chat(messages=messages)

        logger.info(f"âœ… ì±—ë´‡ ì‘ë‹µ ìƒì„± ì™„ë£Œ")

        return ChatResponse(
            answer=ai_response,
            sources=[s.get("id", "") for s in sources],
            tokens_used=0,
            confidence=0.85,
        )

    except Exception as e:
        logger.error(f"âŒ ì±—ë´‡ ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"ì±—ë´‡ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")


@router.get("/health", response_model=dict, summary="ì±—ë´‡ ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬")
async def chat_health():
    """ì±—ë´‡ ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"""
    return {
        "status": "healthy",
        "service": "chat",
        "timestamp": datetime.now().isoformat(),
    }